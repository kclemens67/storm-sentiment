{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## import package bonanza\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import json\n",
    "from pivottablejs import pivot_ui\n",
    "import nltk\n",
    "import unicodedata\n",
    "import twitter_text\n",
    "import re\n",
    "from collections import Counter\n",
    "import regex\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from string import digits\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from configparser import ConfigParser\n",
    "from gensim import corpora, models, similarities\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "from string import digits\n",
    "from geopy.geocoders import Nominatim, GoogleV3\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up sentiment dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# intitialize AFINN dictionary\n",
    "afinn = dict(map(lambda (k,v): (k,int(v)), \n",
    "                     [ line.split('\\t') for line in open(\"/Users/kristinaac/Desktop/insight_weather_project/AFINN/AFINN-111.txt\") ]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intialize postgres server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In Python: Define a database name \n",
    "# final_db, and your username for your computer (CHANGE IT BELOW). \n",
    "dbname = ''\n",
    "username = ''\n",
    "\n",
    "## 'engine' is a connection to a database\n",
    "## Here, we're using postgres, but sqlalchemy can connect to other things too.\n",
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "print engine.url\n",
    "\n",
    "\n",
    "## create a database (if it doesn't exist)\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to convert temperatures and classify weather ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Devising a strategy to rate weather severity based on weather report \n",
    "#gathered from tweet location\n",
    "\n",
    "# converts kelvin to F\n",
    "def kelvin_to_F(t):\n",
    "    return (t - 273.15)* 1.8000 + 32.00\n",
    "\n",
    "\n",
    "# This is the master flat file with all the tweet related data\n",
    "tweet_file = \"/Users/kristinaac/Desktop/insight_weather_project/tweets/160128_full_tweet_data.txt\"\n",
    "\n",
    "# create a sorted list of all weather codes present in the dataset\n",
    "weather_code_list = []\n",
    "with open (tweet_file) as fh:\n",
    "    for line in fh:\n",
    "        weather_code = line.strip().split(\"\\t\")[4]\n",
    "        #print weather_code\n",
    "        weather_code_list.append(weather_code)\n",
    "        line = eval(line.strip().split(\"\\t\")[-1])\n",
    "        \n",
    "        \n",
    "weather_code_list = set(weather_code_list)\n",
    "\n",
    "# 26 catagories of weather; I rated them from 1 to 8, with 8 being most severe. \n",
    "weather_dict = {'741':['Fog','3'],\n",
    "'620': ['light_shower_snow','5'],\n",
    "'601':['snow','6'],\n",
    "'600':['light_snow','5'],\n",
    "'602': ['heavy_snow','7'],\n",
    "'300': ['light_intensity_drizzle','4'],\n",
    "'301':['drizzle','4'],\n",
    "'211': ['thunderstorm','7'],\n",
    "'701': ['mist','2'],\n",
    "'501': ['moderate_rain','6'],\n",
    "'321': ['shower_drizzle','5'],\n",
    "'520' : ['light_intensity_shower_rain','5'],\n",
    "'521': ['shower_rain','6'],\n",
    "'721': ['haze','2'],\n",
    "'803': ['broken_clouds','2'],\n",
    "'802': ['scattered_clouds','2'],\n",
    "'801': ['few_clouds','2'],\n",
    "'800': ['sky_is_clear','1'],\n",
    "'502': ['heavy_intensity_rain','7'],\n",
    "'500': ['light_rain','5'],\n",
    "'804':['overcast_clouds','3'],\n",
    "'761':['dust','3'],\n",
    "'615':['light_rain_and_snow','5'],\n",
    "'302':['heavy_intensity_drizzle','5'],\n",
    "'202':['thunderstorm_with_heavy_rain','8'],\n",
    "'310':['light_intensity_drizzle_rain','4']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to process bounding box coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert bounding box coordinates into lat,long coordinates\n",
    "\n",
    "def get_coordinates(coordinates):\n",
    "    coord3= coordinates[0][0][0]\n",
    "    coord1= coordinates[0][0][1]\n",
    "    coord2 = coordinates[0][1][1]\n",
    "    coord4 = coordinates[0][2][0]\n",
    "        \n",
    "    coords = (coord1,coord2,coord3,coord4)       \n",
    "        \n",
    "    centerx,centery = (numpy.average(coords[:2]),numpy.average(coords[2:]))\n",
    "    centerx = float(centerx)\n",
    "    centery = float(centery)\n",
    "    return(centerx, centery)\n",
    "\n",
    "# weather rating goes from 1-5 1 being the coldest\n",
    "\n",
    "def rate_temp(current_temp):\n",
    "    if int(current_temp) <= 32:\n",
    "        temp_rating = 5\n",
    "    if 32 < int(current_temp) <= 45:\n",
    "        temp_rating = 4\n",
    "    if 45 < int(current_temp) <= 55:\n",
    "        temp_rating = 3\n",
    "    if 55 < int(current_temp) <= 65:\n",
    "        temp_rating = 2\n",
    "    if int(current_temp) > 65:\n",
    "        temp_rating = 1\n",
    "    return temp_rating\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Stopword and weather word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#curated stoplist words\n",
    "\n",
    "stoplist_tw=['amp','get','got','hey','hmm','hoo','hop','iep','let','ooo','par',\n",
    "            'pdt','pln','pst','wha','yep','yer','aest','didn','nzdt','via',\n",
    "            'one','com','new','like','great','make','top','say','would','thanks','thank','going',\n",
    "            'new','use','should','could','see','want',\n",
    "            'while','know','at_user','url','URL','barometer','Sunnyvale','mph','soundcloud','spotify']\n",
    "\n",
    "# cureated wather_term list \n",
    "\n",
    "weather_term_list = ['rain','raindrops','raining','snow','sun','sunny',\n",
    "                     'cloud','cloudy','thunderstorm','thunderstorms',\n",
    "                     'monsoon','thunder','blizzard','icey','warm',\n",
    "                     'hot','cold','weather','showers','flood','stormy','fog',\n",
    "                     'chilly','brr','winter','storm','summer','spring','fall',\n",
    "                     'blizzard2016','Blizzard2106','clouds','snowing','icy','snowball',\n",
    "                     'snowstorm','ice','snowed','snowflakes''jonasblizzard','JonasBlizzard',\n",
    "                     'Jonasblizzard','snowman','rainbow','snowiest', 'winters',\n",
    "                     'snowboarder','snowboard','snowboarding','snowglobe','snowy','thundersnow',\n",
    "                     'rains','snowpocalypse','rainy','snowdays','snowday','snowpack',\n",
    "                     'firstsnow','foggy']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin processing tweets and generating feature lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# intiate emmpty lists that will populate the pd dataframe\n",
    "org_tweet_list =[]\n",
    "search_term_list = []\n",
    "stripped_tweet_list =[]\n",
    "coord_lat_list = []\n",
    "coord_long_list = []\n",
    "location_list = []\n",
    "retweet_list = []\n",
    "time_stamp_list = []\n",
    "tweets_with_emoji = []\n",
    "weather_type_list = []\n",
    "weather_rating_list = []\n",
    "temp_rating_list  = []\n",
    "temp_list = []\n",
    "AFFIN_list = []\n",
    "state_list = []\n",
    "time_list = []\n",
    "weather_related_words_list  = []\n",
    "num_weather_related_words_list = []\n",
    "total_words_list = []\n",
    "\n",
    "# read through the tweet file above and start parsing\n",
    "\n",
    "with open (tweet_file) as fh:\n",
    "    for line in fh:\n",
    "        tweet = line.strip().split(\"\\t\")[0]\n",
    "        \n",
    "        #lower search term case and add to list\n",
    "        search_term = line.strip().split(\"\\t\")[1].lower()\n",
    "        \n",
    "        search_term_list.append(search_term)\n",
    "        \n",
    "        # identify weather code and append rating to list\n",
    "        weather_code = line.strip().split(\"\\t\")[4]\n",
    "        weather_type = weather_dict[weather_code][0]\n",
    "        weather_rating = weather_dict[weather_code][1]\n",
    "        \n",
    "        weather_type_list.append(weather_type)\n",
    "        weather_rating_list.append(weather_rating)\n",
    "        \n",
    "        # identify temp and append rating to list \n",
    "        temp_line = eval(line.strip().split(\"\\t\")[-1])\n",
    "        current_temp = kelvin_to_F(int(temp_line['temp']))\n",
    "        temp_rating = rate_temp(current_temp)\n",
    "        temp_rating_list.append(temp_rating)\n",
    "        temp_list.append(current_temp) \n",
    "        \n",
    "        # add original tweet to tweet list\n",
    "        mydata = json.loads(tweet)\n",
    "        tweet = mydata['text'].strip().encode('ascii', 'ignore')\n",
    "        org_tweet_list.append(tweet)\n",
    "        \n",
    "        # Begin to parse tweet for NPLP \n",
    "\n",
    "        emoji_tweet = tweet#.encode('ascii', 'ignore')\n",
    "        fixed_tweet = tweet.encode('ascii', 'ignore').lower()\n",
    "        fixed_tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',fixed_tweet)\n",
    "        fixed_tweet = re.sub('@[^\\s]+','AT_USER',fixed_tweet) \n",
    "        fixed_tweet = re.sub('[\\s]+', ' ', fixed_tweet)\n",
    "        fixed_tweet = re.sub(r'#([^\\s]+)', r'\\1', fixed_tweet)\n",
    "        fixed_tweet = fixed_tweet.strip('\\'\"')\n",
    "    \n",
    "        # evaluated AFFIN score\n",
    "        AFFIN_score = sum(map(lambda word: afinn.get(word, 0), fixed_tweet.lower().split()))\n",
    "        AFFIN_list.append(AFFIN_score)\n",
    "        \n",
    "        fixed_tweet = tokenizer.tokenize(fixed_tweet.lower())\n",
    "        # remove single and double letter words and stoplists\n",
    "        unigrams = [w for w in fixed_tweet if len(w)==1]\n",
    "        bigrams = [w for w in fixed_tweet if len(w)==2]\n",
    "        stoplist  = set(nltk.corpus.stopwords.words(\"english\") + stoplist_tw\n",
    "                    + unigrams + bigrams)\n",
    "        \n",
    "        fixed_tweet = [token for token in fixed_tweet if token not in stoplist]\n",
    "        # removes random digits\n",
    "        fixed_tweet = [token for token in fixed_tweet if len(token.translate(None,digits)) == len(token)]\n",
    "\n",
    "        #get count of weather related words, total words\n",
    "        weather_related_words = [token for token in fixed_tweet if token in weather_term_list]\n",
    "        num_weather_related_words = len(weather_related_words)\n",
    "        \n",
    "        if weather_related_words == []:\n",
    "            num_weather_related_words = 0\n",
    "            \n",
    "        weather_related_words_list.append(weather_related_words)\n",
    "        num_weather_related_words_list.append(num_weather_related_words)\n",
    "        \n",
    "        \n",
    "        total_words = len([token for token in fixed_tweet])\n",
    "        total_words_list.append(total_words)\n",
    "        \n",
    "        stripped_tweet_list.append(fixed_tweet)\n",
    "\n",
    "        \n",
    "        # get coordinates and append\n",
    "        coordinates = mydata['place']['bounding_box']['coordinates']\n",
    "        coord_1, coord_2, = get_coordinates(coordinates)\n",
    "        coord_lat_list.append(coord_1)\n",
    "        coord_long_list.append(coord_2)\n",
    "\n",
    "        # get location and append\n",
    "        location = mydata['place']['full_name'].encode('ascii', 'ignore').strip()\n",
    "        finer_location = location.strip().split(',')\n",
    "        finer_location = finer_location[len(finer_location)-1]\n",
    "        location_list.append(location)\n",
    "        state_list.append(finer_location)\n",
    "        \n",
    "        # get retweet_count\n",
    "        retweet_count = mydata['retweet_count']\n",
    "        retweet_list.append(retweet_count)\n",
    "        \n",
    "        # get time stamp \n",
    "        time_stamp = mydata['created_at'].encode('ascii', 'ignore').strip()\n",
    "        fix_date = \" \".join(time_stamp.strip().split(\" \")[0:3])\n",
    "        date = time_stamp.strip().split(\" \")[2:4][0]\n",
    "        clock = \"\".join(time_stamp.strip().split(\" \")[2:4][1].split(\":\"))\n",
    "        time = int(\"01%s%s\" %(date,clock))\n",
    "        time_stamp_list.append(fix_date)\n",
    "        \n",
    "        time_list.append(time)\n",
    "\n",
    "        tweets_with_emoji.append(emoji_tweet)\n",
    "\n",
    "# attempt get a count of emojis in each tweet\n",
    "\n",
    "try:\n",
    "    # Wide UCS-4 build\n",
    "    e = re.compile(u'['\n",
    "        u'\\U0001F300-\\U0001F64F'\n",
    "        u'\\U0001F680-\\U0001F6FF'\n",
    "        u'\\u2600-\\u26FF\\u2700-\\u27BF]+', \n",
    "        re.UNICODE)\n",
    "except re.error:\n",
    "    # Narrow UCS-2 build\n",
    "    e = re.compile(u'('\n",
    "        u'\\ud83c[\\udf00-\\udfff]|'\n",
    "        u'\\ud83d[\\udc00-\\ude4f\\ude80-\\udeff]|'\n",
    "        u'[\\u2600-\\u26FF\\u2700-\\u27BF])+', \n",
    "        re.UNICODE)\n",
    "    \n",
    "emoji_dict_list = []\n",
    "for x in tweets_with_emoji: \n",
    "    emoji_list = []\n",
    "    match  = e.search(x)\n",
    "    if match:\n",
    "        emoji_string = match.group()\n",
    "        emoji_list = regex.findall(r'.\\p{Sk}+|\\X',emoji_string)\n",
    "        #print emoji_list\n",
    "        #emoji_dict = Counter(emoji_list)\n",
    "    #emoji_dict_list.append(emoji_dict)\n",
    "\n",
    "#print len(emoji_dict_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add list info to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add it all into a a pd dataframe\n",
    "\n",
    "tweet_data = pd.DataFrame()\n",
    "\n",
    "tweet_data['date_stamp'] = [x for x in time_stamp_list]\n",
    "tweet_data['time'] = [x for x in time_list]\n",
    "tweet_data['location'] = [x for x in location_list]\n",
    "tweet_data['state'] = [x for x in state_list]\n",
    "tweet_data['lat'] = [x for x in coord_lat_list]\n",
    "tweet_data['long'] = [x for x in coord_long_list]\n",
    "tweet_data['org_text'] = [x for x in org_tweet_list]\n",
    "tweet_data['fixed_text'] = [x for x in stripped_tweet_list]\n",
    "tweet_data['retweet_count'] = [x for x in retweet_list]\n",
    "tweet_data['term'] = [x for x in search_term_list]\n",
    "tweet_data['temp'] = [x for x in temp_list]\n",
    "tweet_data['weather'] = [x for x in weather_type_list]\n",
    "tweet_data['weather_rating'] = [x for x in weather_rating_list]\n",
    "tweet_data['temp_rating'] = [x for x in temp_rating_list]\n",
    "tweet_data['affin_score'] = [x for x in AFFIN_list]\n",
    "tweet_data['weather_words'] = [x for x in weather_related_words_list]\n",
    "tweet_data['num_weather_words'] = [x for x in num_weather_related_words_list]\n",
    "tweet_data['total_words'] = [x for x in total_words_list]\n",
    "\n",
    "tweet_data['fixed_text'] = tweet_data['fixed_text'].apply(\" \".join)\n",
    "tweet_data['affin_score'] = tweet_data['affin_score'].apply(float)\n",
    "tweet_data['weather_rating'] = tweet_data['weather_rating'].apply(float)\n",
    "tweet_data['temp'] = tweet_data['temp'].apply(float)\n",
    "tweet_data['retweet_count'] = tweet_data['retweet_count'].apply(float)\n",
    "tweet_data['lat'] = tweet_data['lat'].apply(float)\n",
    "tweet_data['long'] = tweet_data['long'].apply(float)\n",
    "tweet_data['num_weather_words'] = tweet_data['num_weather_words'].apply(float)\n",
    "tweet_data['total_words'] = tweet_data['total_words'].apply(float)\n",
    "tweet_data['time'] = tweet_data['time'].apply(float)\n",
    "tweet_data['date_stamp'] = tweet_data['date_stamp'].apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# filter out duplicates and non-weather related tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# don't include the general tweets that I started collecting near the end\n",
    "tweet_data = tweet_data.loc[tweet_data['term'] != 'general']\n",
    "tweet_data = tweet_data.loc[tweet_data['num_weather_words'] > 0]\n",
    "tweet_data['norm_response_score'] = (tweet_data['affin_score']/4)*tweet_data['weather_rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add dataframe to table in postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## insert data into database from Python (proof of concept - this won't be useful for big data, of course)\n",
    "## df is any pandas dataframe \n",
    "tweet_data.drop_duplicates(subset='org_text', keep='last').to_sql('tweet_weather_table', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Check to make sure the data can be accessed from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In Python: Define a database name (we're using a dataset on births, so I call it \n",
    "# birth_db), and your username for your computer (CHANGE IT BELOW). \n",
    "dbname = ''\n",
    "username = ''\n",
    "\n",
    "## 'engine' is a connection to a database\n",
    "## Here, we're using postgres, but sqlalchemy can connect to other things too.\n",
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "print engine.url\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect:\n",
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username)\n",
    "\n",
    "#sql_query = \"\"\"SELECT location,org_text,temp,weather_rating,norm_response_score, point(%s, %s) <@> point(long, lat)::point AS tweet_distance \n",
    "           #FROM tweet_weather_table WHERE (point(%s, %s) <@> point(long, lat)) < 50 \n",
    "           #ORDER by tweet_distance;\"\"\" %(query_lon,query_lat,query_lon,query_lat)\n",
    "\n",
    "sql_query = \"\"\"SELECT * FROM tweet_weather_table;\"\"\"\n",
    "tweet_data = pd.read_sql_query(sql_query,con)\n",
    "tweet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
